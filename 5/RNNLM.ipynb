{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1.3.3 实现循环神经网络语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.auto import tqdm\n",
    "from utils import BOS_TOKEN,EOS_TOKEN,PAD_TOKEN\n",
    "from utils import load_reuters,save_pretrained,get_loader,init_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建RNNLM的数据处理类，实现训练数据的构建与存取\n",
    "# 使用序列预测的方式构建训练样本\n",
    "class RNNLMDataset(Dataset):\n",
    "    def __init__(self, corpus, vocab):\n",
    "        self.data = []\n",
    "        self.bos = vocab[BOS_TOKEN]\n",
    "        self.eos = vocab[EOS_TOKEN]\n",
    "        self.pad = vocab[PAD_TOKEN]\n",
    "        \n",
    "        for sentence in tqdm(corpus, desc=\"Dataset Construction\"):\n",
    "            # 输入序列，开头加上记号\n",
    "            input = [self.bos]+sentence\n",
    "            # 输出序列，结尾加上记号\n",
    "            target = sentence+[self.eos]\n",
    "            self.data.append((input,target))\n",
    "            \n",
    "    # Subclasses of Dataset must overwrite the 2 functions below\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "    \n",
    "    def collate_fn(self, examples):\n",
    "        # 从独立样本集合中构建batch输入输出\n",
    "        inputs = [torch.tensor(ex[0]) for ex in examples]\n",
    "        targets = [torch.tensor(ex[1]) for ex in examples]\n",
    "        \n",
    "        # 对batch内的样本进行padding使其具有相同长度\n",
    "        inputs = pad_sequence(inputs,batch_first=True,padding_value=self.pad)\n",
    "        targets = pad_sequence(targets,batch_first=True,padding_value=self.pad)\n",
    "        return (inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim):\n",
    "        super(RNNLM,self).__init__()\n",
    "        \n",
    "        # 词嵌入层\n",
    "        self.embeddings = nn.Embedding(vocab_size,embedding_dim)\n",
    "        # LSTM\n",
    "        self.rnn = nn.LSTM(embedding_dim,hidden_dim,batch_first=True)\n",
    "        # 输出层\n",
    "        self.output = nn.Linear(hidden_dim,vocab_size)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        hidden,_ = self.rnn(embeds)\n",
    "        output = self.output(hidden)\n",
    "        log_probs = F.log_softmax(output,dim=2)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "context_size = 2\n",
    "hidden_dim = 128\n",
    "batch_size = 1024\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563ee84c6049476eb69579bee2f311a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset Construction:   0%|          | 0/54716 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 读取文本数据，构建FFNNLM训练数据集（n-grams）\n",
    "corpus, vocab = load_reuters()\n",
    "dataset = RNNLMDataset(corpus, vocab)\n",
    "data_loader = get_loader(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 负对数似然损失函数，忽略pad_token处的损失\n",
    "nll_loss = nn.NLLLoss(ignore_index=dataset.pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLM(\n",
       "  (embeddings): Embedding(31081, 32)\n",
       "  (rnn): LSTM(32, 32, batch_first=True)\n",
       "  (output): Linear(in_features=32, out_features=31081, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建RNNLM，并加载至device\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = RNNLM(len(vocab), embedding_dim, hidden_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Adam优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99943b0fcd5f461e8646ea123590e92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 23.71 GiB (GPU 1; 23.70 GiB total capacity; 845.76 MiB already allocated; 20.57 GiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/data/wyf/NLP-Pre-trained-model-approach/5/RNNLM.ipynb 单元格 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.130/data/wyf/NLP-Pre-trained-model-approach/5/RNNLM.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m inputs, targets \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m batch]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.130/data/wyf/NLP-Pre-trained-model-approach/5/RNNLM.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.130/data/wyf/NLP-Pre-trained-model-approach/5/RNNLM.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m log_probs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.130/data/wyf/NLP-Pre-trained-model-approach/5/RNNLM.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m loss \u001b[39m=\u001b[39m nll_loss(log_probs\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, log_probs\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), targets\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.130/data/wyf/NLP-Pre-trained-model-approach/5/RNNLM.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda/envs/wyf/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/data/wyf/NLP-Pre-trained-model-approach/5/RNNLM.ipynb 单元格 10\u001b[0m in \u001b[0;36mRNNLM.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.130/data/wyf/NLP-Pre-trained-model-approach/5/RNNLM.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.130/data/wyf/NLP-Pre-trained-model-approach/5/RNNLM.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m hidden,_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(embeds)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.130/data/wyf/NLP-Pre-trained-model-approach/5/RNNLM.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.130/data/wyf/NLP-Pre-trained-model-approach/5/RNNLM.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m log_probs \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlog_softmax(output,dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.130/data/wyf/NLP-Pre-trained-model-approach/5/RNNLM.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m log_probs\n",
      "File \u001b[0;32m~/miniconda/envs/wyf/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/wyf/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda/envs/wyf/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 23.71 GiB (GPU 1; 23.70 GiB total capacity; 845.76 MiB already allocated; 20.57 GiB free; 1024.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(inputs)\n",
    "        loss = nll_loss(log_probs.view(-1, log_probs.shape[-1]), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss: {total_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pretrained(vocab, model.embeddings.weight.data, \"rnnlm.vec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49cb93f377a7abe7414b7b0f21fb3017538004a126cf690fb524202736b7fb92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
